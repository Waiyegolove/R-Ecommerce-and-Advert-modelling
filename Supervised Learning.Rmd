---
title: "Supervised Learning in R"
author: "Waiyego Mburu"
date: "09/07/2021"
output: html_document
---

## **DATA UNDERSTANDING**

To identify which kind of individuals(profiles) are most likely to click on ads related to an online cryptography
course if it were advertised on a blog given data on a related course that was advertised on the same blog
some time ago.

## **METRIC OF SUCCESS**

If we create a model that has a high accuracy in predicting which group of people are highly likely to click on adverts for an online cryptography course, the initiative will be a success. The ability to come up with the best fits for these initiatives will be critical to their success.

## **CONTEXT**

A Kenyan entrepreneur has created an online cryptography course and would want to advertise it on her blog. She currently targets audiences originating from various countries. In the past, she ran ads to advertise a related course on the same blog and collected data in the process. She would now like to employ your services as a Data Science Consultant to help her identify which individuals are most likely to click on her ads. 

## **EXPERIMENTAL DESIGN**

* Import the relevant libraries that we will use in our analysis
* Read and explore the dataset we will use for our project
* Define the appropriateness of the available data with regards to the project
* Find and deal with outliers, anomalies, and missing data within the dataset.
* Perform univariate and bivariate analysis while recording our observations.
* Implementing the solution using various supervised machine learning algorithms
* Challenging the Solution
* Follow up Questions

## **DATA RELEVANCE**
Our dataset is very relevant since it has most of the factors considered before rolling out of an advert. These factors include age group, site, traffic on the site.

## **READING THE DATASET**
```{r}
advertising = read.csv("http://bit.ly/IPAdvertisingData")
```

##### Checking the first 6 entries
```{r}
head(advertising)
```
##### Checking the last 6 entries
```{r}
tail(advertising)
```
##### Checking the shape of our dataset
```{r}
dim(advertising)
```

##### Checking for duplicates and unique values
```{r}
duplicates <- advertising[duplicated(advertising),]
duplicates
```
There are no duplicates in our dataset.

##### Getting information on our datatypes
```{r}
str(advertising)
```

## Checking for null values per column
```{r}
colSums(is.na(advertising))
```
Our dataset has no missing data hence is complete and of good quality.

## Checking for outliers in our numerical variables

```{r}
Time.onsite <- advertising$Daily.Time.Spent.on.Site
Age <- advertising$Age
Income <- advertising$Area.Income
Internet <- advertising$Daily.Internet.Usage
Topic <- advertising$Ad.Topic.Line
City <- advertising$City
Gender <- advertising$Male
Country <- advertising$Country
Clicked.on.Ad <- advertising$Clicked.on.Ad
```

```{r}
boxplot(Time.onsite)
```

```{r}
boxplot(Age)
```

```{r}
boxplot(Income)
```
```{r}
boxplot.stats(Income)$out
```

As can be seen the area income column has outliers. These outliers can be explained since there are other factors at play like demographics and varied income rates in different countries. Therefore, we do not remove the outliers.

```{r}
boxplot(Internet)
```


## **Exploratory Data Analysis**

## Univariate Analysis

### Measures of central Tendency (mean ,median,mode)

#### **Mean**
```{r}
Time.onsite.mean = mean(Time.onsite)
Time.onsite.mean
```
```{r}
Age.mean = mean(Age)
Age.mean
```
```{r}
Income.mean = mean(Income)
Income.mean
```
```{r}
Internet.mean = mean(Internet)
Internet.mean
```

#### **Mode**
```{r}
getmode = function(v){
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]  
}
```
```{r}
Time.onsite.mode = getmode(Time.onsite)
Time.onsite.mode
```
```{r}
Age.mode = getmode(Age)
Age.mode
```
```{r}
Income.mode = getmode(Income)
Income.mode
```
```{r}
Internet.mode = getmode(Internet)
Internet.mode
```
```{r}
Country.mode = getmode(Country)
Country.mode
```

```{r}
City.mode = getmode(City)
City.mode
```
1. The most occurring age among our users is 31 years.
2. The City with the most repeat users is Lisamouth.
3. The country that’s most repeated is Czech Republic.
4. Most of our respondents were female.

#### **Median**
```{r}
Time.onsite.median = median(Time.onsite)
Time.onsite.median
```

```{r}
Age.median = median(Age)
Age.median
```

```{r}
Income.median = median(Income)
Income.median
```

```{r}
Internet.median = median(Internet)
Internet.median
```

### Measures of dispersion(Variance, SD)

#### **Variance**
```{r}
Time.onsite.variance = var(Time.onsite)
Time.onsite.variance
```
```{r}
Age.variance = var(Age)
Age.variance
```
```{r}
Income.variance = var(Income)
Income.variance
```
```{r}
Internet.variance = var(Internet)
Internet.variance
```

#### **Standard Deviation**
```{r}
Time.onsite.standard.deviation = sd(Time.onsite)
Time.onsite.standard.deviation
```
```{r}
Age.standard.deviation = sd(Age)
Age.standard.deviation
```
```{r}
Income.standard.deviation = sd(Income)
Income.standard.deviation
```
```{r}
Internet.standard.deviation = sd(Internet)
Internet.standard.deviation
```

### Range

```{r}
range(Time.onsite)
```

```{r}
range(Age)
```

```{r}
range(Income)
```

```{r}
range(Internet)
```
1. The minimum time spent on the site is 32.60 while the maximum time spent on the site is 91.43
2. The least age of the users is 19 while the largest age is 61
3. Area income ranges between 13996.5 and 79484.6

### Skewness and Kurtosis

#### **Skewness**
```{r}
# Load the e1071 library that allows us to compute skewness and kurtosis
library(e1071)
```

```{r}
Time.onsite.skew = skewness(Time.onsite)
Time.onsite.skew
```

```{r}
Age.skew = skewness(Age)
Age.skew
```

```{r}
Income.skew = skewness(Income)
Income.skew
```

```{r}
Internet.skew = skewness(Internet)
Internet.skew
```

#### **Kurtosis**

```{r}
Time.onsite.kurt = kurtosis(Time.onsite)
Time.onsite.kurt
```

```{r}
Age.kurt = kurtosis(Age)
Age.kurt
```

```{r}
Income.kurt = kurtosis(Income)
Income.kurt
```

```{r}
Internet.kurt = kurtosis(Internet)
Internet.kurt
```

## Plotting Bar Charts of categorical variables

```{r}
Gender.distribution <- table(Gender)
label <- c("Female","Male")
barplot(Gender.distribution,names.arg = label,main = "Gender Distribution")
```
```{r}
Clicked.on.Ad.frequency = table(Clicked.on.Ad)
label <- c("No","Yes")
barplot(Clicked.on.Ad.frequency,names.arg = label,main = "Clicked on AD Distribution")

```
```{r}
# top 10 countries that accrue high average income
avg.country = aggregate(Income, by = list(Country), FUN = mean)
cou.top10 <- head(avg.country[order(avg.country$x,decreasing = TRUE), ],10)
barplot(cou.top10$x,main = "COUNTRIES THAT ACCRUE HIGH INCOME",
        xlab = "Area Income",
        density = 80,
        las = 1,
        names = cou.top10$Group.1,
        horiz = TRUE)
```

```{r}
# Creating a dataframe for those who clicked the ad
Ads <- advertising[Clicked.on.Ad == 1,]
```

```{r}
## distribution for countries
topcountries<- head(sort(table(Ads$Country),decreasing=TRUE),n=10)
barplot(topcountries,las=1, main="Top Countries",horiz=TRUE)
```

```{r}
hist(Time.onsite,xlab = "Daily Time Spent on Site", main = "Histogram of Daily Time Spent on Site")
```

```{r}
hist(Age,xlab = "Age of User", main = "Histogram of Age of User")
```

```{r}
hist(Ads$Age,xlab = "Age of User", main = "Histogram of Age of User")
```

```{r}
hist(Income, xlab = "Area Income",main = "Histogram of Area Income")
```
```{r}
hist(Internet,xlab = "Daily Internet Usage", main = "Histogram of Daily Internet Usage")
```
```{r}
library(psych)
```


```{r}
# Summary of measures of central tendency and dispersion
Description = describe(advertising)
Description
```

## BIVARIATE ANALYSIS
```{r}
### Covariance between the Area_Income and Age and Clicked_on_Ad
cov(Income, Clicked.on.Ad)
```
```{r}
# Covariance between age and daily time on site
cov(Age,Time.onsite)
```
```{r}
### Covariance between the Age and Clicked on Ad
cov(Age,Clicked.on.Ad)
```

```{r}
## Covariance between the Age and Internet Usage
cov(Age,Internet)
```
```{r}
## Correlation between Age and Daily time onsite 
cor(Age,Time.onsite)
```
```{r}
## Correlation between age and internet
cor(Age, Internet)
```
```{r}
#Plotting the above correlation matrix as a heatmap of whole dataset
#correlation
res <- cor(advertising[1:4],method = "pearson")
```

```{r}
#import library used to plot correlation matrix
library(corrplot)
```

```{r}
## corrplot 0.84 loaded
corrplot(res, type = "lower", order = "hclust", tl.col = "red", tl.srt = 50)
```

```{r}
plot(Time.onsite,Age, xlab ="Daily.Time.Spent.On.Site", ylab ="Age")

```

```{r}
plot(Age,Income, xlab ="Age", ylab ="Income")
```

## **IMPLEMENTING THE SOLUTION**
```{r}
# selecting numerical columns
df <- advertising[,c(1:4,7,10)]
head(df)
```

```{r}
library(caTools)
# Splitting data into train
# and test data
split <- sample.split(df, SplitRatio = 0.8)
train <- subset(df, split == "TRUE")
test <- subset(df, split == "FALSE")
  
# Feature Scaling
train_scale <- scale(train[, 1:5])
test_scale <- scale(test[, 1:5])
```


### **LOGISTIC REGRESSION**
```{R}
Clicked.on.Ad <- train$Clicked.on.ad
Age <- train$Age
Internet <- train$Daily.Internet.Usage
Time.onsite <- train$Daily.Time.Spent.on.Site
Area.Income<- train$Area.Income
Gender <- train$Male
```


```{r}
model <- glm(Clicked.on.Ad ~ Age + Internet + Time.onsite + Area.Income + Gender, data=train, family = binomial(link = 'logit'))
```


```{r}
summary(model)
```
The negative coefficient for this predictor suggests that all other variables being equal, the male respondent is least likely to click on ad. Remember that in the logit model, the response variable is log odds: ln(odds) = ln(p/(1-p)) = (a*x1+b*x2) + … + z*xn. Since male is a dummy variable, being male reduces the log odds by 0.06847 while a unit increase in age increases the log odds by 0.01648.

We run the anova() function on the model to analyze the table of deviance

```{r}
anova(model, test="Chisq")

```

Analysis of Deviance Table
Model: binomial, link: logit
Response: Survived
Terms added sequentially (first to last)

         Df Deviance Resid. Df Resid. Dev  Pr(>Chi)    
NULL                       799    1065.39              
Pclass    1   83.607       798     981.79 < 2.2e-16 *
Sex       1  240.014       797     741.77 < 2.2e-16 *
Age       1   17.495       796     724.28 2.881e-05 *
SibSp     1   10.842       795     713.43  0.000992 *
Parch     1    0.863       794     712.57  0.352873    
Fare      1    0.994       793     711.58  0.318717    
Embarked  2    2.187       791     709.39  0.334990    
---
Signif. codes:  0 ‘*’ 0.001 ‘*’ 0.01 ‘’ 0.05 ‘.’ 0.1 ‘ ’ 1


The difference between the null deviance and the residual deviance shows how our model is doing against the null model (a model with only the intercept). The wider this gap, the better. Analyzing the table we can see the drop in deviance when adding each variable one at a time. Again, adding Age, Internet and Time spent on site significantly reduces the residual deviance. The other variables seem to improve the model less. A large p-value here indicates that the model without the variable explains more or less the same amount of variation. Ultimately what you would like to see is a significant drop in deviance.


```{r}
library(pscl)
```

```{r}
pR2(model)
```
  
```{r}  
Clicked.on.Ad <- test$Clicked.on.ad
Age <- test$Age
Internet <- test$Daily.Internet.Usage
Time.onsite <- test$Daily.Time.Spent.on.Site
Area.Income<- test$Area.Income
Gender <- test$Male
```


```{r}
fitted.results <- predict(model,newdata=subset(test,select=c(1:6)),type='response')
fitted.results <- ifelse(fitted.results > 0.5,1,0)
misClasificError <- mean(fitted.results != test$Clicked.on.Ad)
print(paste('Accuracy',1-misClasificError))
```

The 0.98 accuracy on the test set is quite a good result. However, keep in mind that this result is somewhat dependent on the manual split of the data made earlier, therefore if you wish for a more precise score, you would be better off running some kind of cross validation such as k-fold cross validation.

### **K-NEAREST NEIGHBOUR CLASSIFIER**
```{r}
library(class)
library(caret)
```

```{r}
# Fitting KNN Model 
# to training dataset
knn <- knn(train = train_scale,
           test = test_scale,
            cl = train$Clicked.on.Ad,
            k = 32)
```

```{r}
# Confusion Matrix
cm <- table(test$Clicked.on.Ad,knn)
confusionMatrix(cm)
```

### **NAIVE BAYES CLASSIFIER**
```{R}
# Loading package
library(e1071)
library(caTools)
library(caret)
```

```{r}
# Fitting Naive Bayes Model to the training dataset
set.seed(120)  # Setting Seed
Naivebayes <- naiveBayes(Clicked.on.Ad ~ ., data = train)

```

```{r}
# Predicting on test data'
y_pred <- predict(Naivebayes, newdata = test)
# Confusion Matrix
cm <- table(test$Clicked.on.Ad,y_pred)

# Model Evauation
confusionMatrix(cm)
```

### **RANDOM FOREST CLASSIFIER**
```{r}
# Loading package
library(caTools)
library(randomForest)
```

```{r}
# Fitting Random Forest to the train dataset
set.seed(120)  # Setting seed
rf <- randomForest(as.factor(Clicked.on.Ad) ~ ., 
                        data = train, 
                        importance = TRUE,
                        proximity = TRUE)
```

```{r}
print(rf)
```

```{r}
# Predicting the Test set results
y_pred = predict(rf, test)
```

```{r}
# Confusion Matrix
confusion_mtx = table( y_pred,test[, 6])
confusion_mtx
```

```{r}
# Plotting model
plot(rf)
```

```{r}
# Importance plot
importance(rf)
```

```{r}
# Variable importance plot
varImpPlot(rf)
```

As can be seen according to Random Forest Classifier the most important feature in our model is daily internet usage followed by daily time spent on site. The least important faeture is Gender.

## **Conclusion**
The model accuracies are as follows:
 * Logistic Regression: **98.2%**
 * K-Nearest Neighbors: **96.1%**
 * Naive Bayes: **98.2%**
 * Random Forest: **96.4%**
 
## **Challenging the Solution**
To challenge our solution we try modelling using the support vectors classifier.

### **SUPPORT VECTOR CLASSIFIER**
```{r}
# 
library(e1071)
  
classifier = svm(formula = Clicked.on.Ad ~ .,
                 data = train,
                 type = 'C-classification',
                 kernel = 'linear')
```

```{r}
# Predicting the Test set results
y_pred = predict(classifier, newdata = test[-6])
```

```{r}
# Making the Confusion Matrix
cm = table(test[, 6], y_pred)
cm
```

```{r}
confusionMatrix(cm)
```

## **RECOMMENDATIONS**

The most suitable models for prediction are Logistic regression, Naive Bayes Classifier and Support Vector Classifier Since they achieve the highest accuracies.

## **Follow up questions**

a). Did we have the right data?
Yes, we had the right Data because the relevant independent variables have a high accuracy in predicting our dependent variable

b). Do we need other data to answer our question?
Yes, we need more Data to ensure we haven't over fitted our data.

c). Did we have the right question?
Yes, we have the right question.
